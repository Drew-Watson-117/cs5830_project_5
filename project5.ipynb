{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Determining Whether a Song is Explicit\n",
    "\n",
    "Our goal is to create a Naive Bayes Classifier model for determining whether a song is explicit given its lyrics. Our data was obtained using the Musixmatch API. \n",
    "\n",
    "We got the 13 top artists in the US, then for each artist we got all their songs. Then, we got the lyrics for each of those songs and whether the song is explicit. We then wrote that data to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import ast\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "To collect the data, we used the Python wrapper for the Musixmatch API. The code used to obtain the data is contained in `getData.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To prepare the data, we had to stem the song lyrics and remove stop words using `nltk`. Then, we had to use a `CountVectorizer` to convert the lyrics into a matrix of word counts. Finally, we had to group features and labels into a format compatable with `nltk.NaiveBayesClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read in the data. Then, we stem it and remove stop words to make things more simple for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv(\"data/tracks.csv\",index_col=0)\n",
    "tracks.drop_duplicates(inplace=True)\n",
    "tracks.dropna(inplace=True)\n",
    "tracks.reset_index(inplace=True)\n",
    "del tracks[\"index\"]\n",
    "\n",
    "# Stemming words and removing stop words\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to stem lyrics\n",
    "def stemLyrics(stop_words):\n",
    "    stemmedLyricsList = []\n",
    "    for lyrics in tracks['lyrics']:\n",
    "        lyricsList = lyrics.split(\" \")\n",
    "        stemmedLyrics = [stemmer.stem(word) for word in lyricsList if word.lower() not in stop_words]\n",
    "        stemmedLyrics = ' '.join(stemmedLyrics)\n",
    "        stemmedLyricsList.append(stemmedLyrics)\n",
    "    return stemmedLyricsList\n",
    "\n",
    "stemmedLyrics = stemLyrics(stopWords)\n",
    "tracks = tracks.assign(stemmed_lyrics=stemmedLyrics)\n",
    "tracks\n",
    "\n",
    "stemmed_lyrics = tracks['stemmed_lyrics']\n",
    "y = tracks['explicit']\n",
    "\n",
    "print(len(tracks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we supply the lyrics to a count vectorizer to get the counts of each word present. Then, we put our data in a form which `nltk` is happy with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vec = list(vectorizer.fit_transform(stemmed_lyrics).toarray())\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tracks = tracks.assign(vectorized_lyrics=vec)\n",
    "tracks\n",
    "\n",
    "featuresets = []\n",
    "# featursets have form ({\"feature1\": count, \"feature2\": count, ...}, isExplicit)\n",
    "for i in range(len(y)):\n",
    "    featureX = {}\n",
    "    for j in range(len(words)):\n",
    "        featureX[words[j]] = (tracks['vectorized_lyrics'].iloc[i])[j]\n",
    "    featuresets.append((featureX, y.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model\n",
    "\n",
    "Now, we can define a function which will train a Naive Bayes Classifier on supplied data, and then test the classifier and return both the classifier and the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes classifier function\n",
    "def naive_bayes(featuresets, printMetrics=True, showConfusionMatrix=False):\n",
    "\n",
    "    # Split into training and testing\n",
    "    trainSet, testSet = train_test_split(featuresets)\n",
    "    # Train the classifier\n",
    "    classifier = nltk.NaiveBayesClassifier.train(trainSet)\n",
    "\n",
    "    # Predict the test data\n",
    "    testY = list(list(zip(*testSet))[1])\n",
    "    test_features = list(list(zip(*testSet))[0])\n",
    "    predictY = [classifier.classify(features) for features in test_features]\n",
    "\n",
    "    # Get metrics for classifier (precision, recall, fscore, support)\n",
    "    metrics = {}\n",
    "    p,r,f,s = precision_recall_fscore_support(testY,predictY)\n",
    "\n",
    "    metrics[\"precision\"] = p\n",
    "    metrics[\"recall\"] = r\n",
    "    metrics[\"f-score\"] = f\n",
    "    metrics[\"support\"] = s\n",
    "\n",
    "    if printMetrics:\n",
    "        for metric in metrics.keys():\n",
    "            print(f\"{metric}: {metrics[metric]}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    if showConfusionMatrix:\n",
    "        labels = [\"Not Explicit\", \"Explicit\"]\n",
    "        confusionMatrix = confusion_matrix(testY,predictY)\n",
    "        display = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=labels)\n",
    "        display.plot()\n",
    "\n",
    "    return classifier, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `naive_bayes` function to get a classifier and metrics for the dataset. Then, display the most informative features for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run naive bayes function\n",
    "classifier, metrics = naive_bayes(featuresets,showConfusionMatrix=True)\n",
    "# classifier, metrics = naive_bayes(data['featureset'],showConfusionMatrix=True)\n",
    "classifier.show_most_informative_features(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "We will now only take a subset of the features and see how the model performs. This will allow models to train quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unimportant features\n",
    "num_most_informative = 30\n",
    "most_informative_features = [featureName for (featureName,_) in classifier.most_informative_features(num_most_informative)]\n",
    "informative_featuresets = []\n",
    "for (featureDict,isExplicit) in featuresets:\n",
    "    newFeatureDict = {}\n",
    "    for word in featureDict.keys():\n",
    "        if word in most_informative_features:\n",
    "            newFeatureDict[word] = featureDict[word]\n",
    "    informative_featuresets.append((newFeatureDict,isExplicit))\n",
    "informative_featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supply new dataset with less features to `naive_bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new classifier based only on most informative features\n",
    "informative_classifier, informative_metrics = naive_bayes(informative_featuresets,showConfusionMatrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the metrics displayed above, recall is very low. The confusion matrix tells us that dropping less informative features results in a large number of false negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "We will now define a Monte Carlo cross validation function which will train `n` naive bayes classifiers, each with randomly chosen train/test subsets. The function will return the average metrics for the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which does Monte Carlo\n",
    "def MonteCarloCrossValidation(featuresets,n):\n",
    "    metrics_averages = {\"precision\": 0, \"recall\": 0, \"f-score\": 0, \"support\": 0}\n",
    "    for _ in range(n):\n",
    "        _, metrics = naive_bayes(featuresets,printMetrics=False)\n",
    "        for metric in metrics.keys():\n",
    "            metrics_averages[metric] += metrics[metric]\n",
    "    for metric in metrics_averages.keys():\n",
    "        metrics_averages[metric] = metrics_averages[metric]/n\n",
    "\n",
    "    for metric in metrics.keys():\n",
    "        print(f\"average {metric}: {metrics_averages[metric]}\")\n",
    "    return metrics_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Monte Carlo cross validation on the data with all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of classifier using all features\n",
    "metrics_averages_all = MonteCarloCrossValidation(featuresets,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run Monte Carlo cross validation on the data with only the most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_averages_most_informative = MonteCarloCrossValidation(informative_featuresets,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation tests above support the claim that dropping less informative features results in a large amount of false negatives and therefore low recall. We can see that leaving in all the features is a good idea."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
